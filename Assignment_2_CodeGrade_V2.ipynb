{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-0-markdown",
      "metadata": {},
      "source": [
        "# Classification using Gaussian distributions\n",
        "\n",
        "For this weeks assignment, you will implement a *classification* algorithm, which is the other main type of *supervised learning* algorithm (as opposed to *regression*, from last week). Specificially, you will build a *Bayes classifier* using *Gaussian distributions* to estimate the likelihood of continuous variables.\n",
        "\n",
        "For this we will work with a classic dataset from machine learning; Fisher's Iris dataset. The dataset contains the measurements of *length* and *width* of the *sepals* and *petals* of 150 flowers.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/78/Petal-sepal.jpg\" width=250>\n",
        "\n",
        "Using these 4 attributes (*length* and *width* of both *sepals* and *petals*), the flowers should then be classified as as one of 3 species of Iris flower:\n",
        "\n",
        "* Iris setosa\n",
        "* Iris versicolor\n",
        "* Iris virginica\n",
        "\n",
        "This dataset is such a classic example that it is even included in machine learning libraries such as [scikit-learn](http://scikit-learn.org/stable/index.html). For now, we will only use it to load in the dataset, but in later assignments we will use more of its capabilities. The code below should store and show the dataset as the variable `iris`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CodeGrade Tag init1\n",
        "import math\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sets the random seed for Numpy such that Codegrade will have the same randomization\n",
        "# DO NOT CHANGE THE LINE BELOW WHEN HANDING IN TO CODEGRADE!\n",
        "RANDOM_SEED = 4\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "print(iris)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-2-markdown",
      "metadata": {},
      "source": [
        "## Separate and plot the data [1 pt]\n",
        "\n",
        "The `iris` variable now contains an object with all sorts of information about the dataset. The 2 most important atributes are *target* and *data* containing respectively the labels and the data points for the 150 flowers. An attribute of this data object can be accessed with **.** (e.g., `iris.data`).\n",
        "\n",
        "Use this to create the matrix $X$ and the vector $R$. Then divide the matrix $X$ into 3 parts, `C0`, `C1` and `C2`, each containing the data for a different class, as labeled by the vector $R$. *Hint: Some clever [indexing](https://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays) might help here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def separate_data(dataset):\n",
        "    \"\"\"\n",
        "    Separates the dataset into X & R components.\n",
        "\n",
        "    Input:\n",
        "    dataset - The data from Scipy\n",
        "\n",
        "    Output:\n",
        "    - The data & targets from the dataset.\n",
        "    \"\"\"\n",
        "    # SOLUTION HERE\n",
        "    raise NotImplementedError\n",
        "\n",
        "def split_data(X, R):\n",
        "    \"\"\"\n",
        "    Splits the data into class-specific matrices C0, C1 and C2.\n",
        "\n",
        "    Input:\n",
        "    X - The input data from the dataset.\n",
        "    R - The targets from the dataset.\n",
        "\n",
        "    Output:\n",
        "    - 3 separate arrays for each class C.\n",
        "    \"\"\"\n",
        "    # SOLUTION HERE\n",
        "    raise NotImplementedError\n",
        "\n",
        "# SOLUTION HERE\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-4-markdown",
      "metadata": {},
      "source": [
        "Lets plot this data, so we can see what it looks like. We have prepared some code below that plots a so-called scatter matrix. This is a matrix of scatter plots that can be used to easily identify correlations and see the seperability of the dataset. The diagonals of this plot show how the data is distributed for every *single* feature, while the non-diagonals show scatter plots using *two* of the features as the $x$ and $y$ axis, with all the different possible configurations shown in the matrix.\n",
        "\n",
        "The different colors here correspond to the different classes of Iris, which is the same separation you just made, dividing the data points into $C_0$, $C_1$ and $C_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CodeGrade Tag ignore seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "df_iris = sns.load_dataset('iris')\n",
        "sns.pairplot(df_iris, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-6-markdown",
      "metadata": {},
      "source": [
        "Looking at these plots, it should be quite clear that the Setosa species is more easily separated than Versicolor and Virginica. Also, it seems that quite a few of the variable combinations are correlated with each other.\n",
        "\n",
        "## Inspect and estimate the correlations [1pt]\n",
        "\n",
        "One measure of how variables are correlated is the covariance. The covariance between two jointly distributed real-valued random variables $X$ and $Y$  is defined as the expected product of their deviations from their individual expected values:\n",
        "\n",
        "\\begin{align}\n",
        "\\operatorname{cov}(X, Y)\n",
        "&= \\operatorname{E}\\left[\\left(X - \\operatorname{E}\\left[X\\right]\\right) \\left(Y - \\operatorname{E}\\left[Y\\right]\\right)\\right] \\\\\n",
        "&= \\operatorname{E}\\left[X Y - X \\operatorname{E}\\left[Y\\right] - \\operatorname{E}\\left[X\\right] Y + \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right]\\right] \\\\\n",
        "&= \\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right] + \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right] \\\\\n",
        "&= \\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right],\n",
        "\\end{align}\n",
        "\n",
        "When covariance is positive, this means that the variables are positively correlated: i.e. when one variable increases in value, it is probable that the other variable also increases. When covariance is negative, this means that the variables are negatively correlated: i.e. when one variable increases in value, it is probable that the other variable decreases. When covariance is near zero, there is no correlation between the variables.\n",
        "\n",
        "In the cell below we have provided you with a small bit of code that should help you to fit a first Gaussian distribution to the Versicolor part of the data. The code generates some samples from a 2 dimensional Gaussian distribution, using a *mean vector* and a *covariance matrix* and plots those together with the data. The mean vector and both the standard deviations have already been provided, but to complete the covariance matrix, the covariance between the two features should also be set.\n",
        "\n",
        "Together, this is enough to start generating samples, and the more the different distributions overlap, the better the fit of the generated distribution. Manually change the variable `covar` such that the distribution of the generated samples overlaps as much as possible with the real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_class = 1\n",
        "x_feature = 0\n",
        "y_feature = 2\n",
        "\n",
        "std_x = 0.51\n",
        "std_y = 0.47\n",
        "\n",
        "# CHANGE ME #\n",
        "covar = 0.0\n",
        "#############\n",
        "\n",
        "cov = np.array([[std_x**2, covar], [covar, std_y**2]])\n",
        "\n",
        "mean = [np.mean(C1[:, x_feature]), np.mean(C1[:, y_feature])]\n",
        "\n",
        "samples = np.random.multivariate_normal(mean, cov, size=100)\n",
        "\n",
        "plt.scatter(C1[:, x_feature], C1[:, y_feature], label='Real data')\n",
        "plt.scatter(*zip(*samples), label='Generated samples')\n",
        "plt.title(iris.target_names[target_class])\n",
        "plt.xlabel(iris.feature_names[x_feature])\n",
        "plt.ylabel(iris.feature_names[y_feature])\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "print(cov)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-8-markdown",
      "metadata": {},
      "source": [
        "*How do the values of `std_x` and `std_y` affect the shape of the resulting distribution?*\n",
        "\n",
        "__YOUR ANSWER HERE__\n",
        "\n",
        "\n",
        "*How are the sepal length and petal length correlated for the Versicolor class?*\n",
        "\n",
        "__YOUR ANSWER HERE__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-9-markdown",
      "metadata": {},
      "source": [
        "## Validation sets [1 pt]\n",
        "\n",
        "Now that we have an idea what our dataset looks like, our goal is to create a model that will predict the class (in this case iris species) based on the other variables. In order to evaluate how well the model fits, we will also need a validation set where we can test some of our predictions. For this we will again split the data into a training and a validation set. Fill in the function below, you can use your function from last week as inspiration. This time, rather than randomly shuffling your data at each run, please use the `fixed_order` given below to shuffle your data accordingly.\n",
        "\n",
        "Now use this function on the data from each of the 3 classes, using a ratio of $0.7$ for each. This should result in 3 different training sets and 3 different validation sets. Be sure to name them all differently, so you can still use them all later in the assignment.\n",
        "\n",
        "*Hint: In order to guarantee that the results of the shuffle are deterministic (and will not fail the autograder), please use the newer (default) [RNG Generator from Numpy](https://numpy.org/doc/2.3/reference/random/generated/numpy.random.Generator.shuffle.html#numpy.random.Generator.shuffle) to shuffle your data. The old method `np.random.shuffle()` appears to lead to inconsistent results. Use the RANDOM_SEED constant defined at the top of the file for the generator. Remember to make a COPY of the data and not shuffling in-place!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_split(data, ratio, seed):\n",
        "    \"\"\"\n",
        "    Splits the data into a train & validation set.\n",
        "\n",
        "    Input:\n",
        "    data - A Numpy array containing the input data.\n",
        "    ratio - A fixed ratio to use when dividing the data into two sets.\n",
        "    seed - The seed set to ensure equal shuffling (DO NOT CHANGE WHEN HANDING IN TO CODEGRAGE!)\n",
        "\n",
        "    Output:\n",
        "    - 2 Numpy arrays, one for training & 1 for validation.\n",
        "    \"\"\"\n",
        "    # YOUR SOLUTION HERE\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# YOUR SOLUTION HERE (invoke the function using RANDOM_SEED)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-11-markdown",
      "metadata": {},
      "source": [
        "## Univariate model [1 pt]\n",
        "\n",
        "Looking at the scatter matrix of the data from the previous section, you might conclude that separating the different classes would be a lot easier based on the petal data (3rd and 4th variable) than on the sepal data (1st and 2nd variable), as it easier to distinguish the different clusters in that plot. In fact, for now we will only focus on 1 variable, the **petal length (3rd variable)**, as it looks like it might be useful just on its own and this will simplify the model a lot.\n",
        "\n",
        "Using the training data from each of 3 classes, compute the mean and standard deviation for the *petal length* attribute for each class. Use the variable names given in the print statements. The Maximum Likelihood Estimators for these are given by\n",
        "\n",
        "(4.8a) $$m = \\frac{\\sum_{t=1}^Nx^t}{N}$$\n",
        "\n",
        "(4.8b) $$s = \\sqrt{\\frac{\\sum_{t=1}^N(x^t - m)^2}{N}}$$\n",
        "\n",
        "You can also use the built-in *Numpy* functions for [mean](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and [standard deviation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html). Print the results for each class and make sure they make sense in comparison to the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean(C: np.array, variable: int):\n",
        "    \"\"\"\n",
        "    Computes the mean of the input array.\n",
        "\n",
        "    Input:\n",
        "    C - Numpy array\n",
        "    variable - Int\n",
        "\n",
        "    Output:\n",
        "    - Float mean value\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def compute_std(C: np.array, variable: int):\n",
        "    \"\"\"\n",
        "    Computes the standard deviation of the input array.\n",
        "\n",
        "    Input:\n",
        "    C - Numpy array\n",
        "    variable - Int\n",
        "\n",
        "    Output:\n",
        "    - Float STD value\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# print results (match the variable names shown & uncomment the lines)\n",
        "#print(\"C0: mean=%.3f, std=%.3f\" %(mean_C0_train, std_C0_train))\n",
        "#print(\"C1: mean=%.3f, std=%.3f\" %(mean_C1_train, std_C1_train))\n",
        "#print(\"C2: mean=%.3f, std=%.3f\" %(mean_C2_train, std_C2_train))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-13-markdown",
      "metadata": {},
      "source": [
        "## Probability density function [2 pts]\n",
        "\n",
        "The probability density function for a Gaussian distribution is defined as\n",
        "\n",
        "(4.7) $$p(x|\\mu, \\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
        "\n",
        "where $X$ is Gaussian (normal) distributed with mean $\\mu$ and variance $\\sigma^2$, denoted as $\\mathcal{N}(\\mu,\n",
        "\\sigma^2$).\n",
        "\n",
        "That means that if we have estimates for $\\mu$ and $\\sigma$, we can compute the probability density for a specific value $x$. Implement this in the function below.\n",
        "\n",
        "You have already made estimates for $\\mu$ and $\\sigma$ for the *petal length* for each of the 3 classes, so we can now define PDFs for each separate class. Plot the 3 functions using [linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) for a range of x-values ($0\\leq x\\leq 7$, with 1000 generated samples) and apply the PDF functions to obtain the y-values. Use the previous plots to estimate a sensible range for *linspace* and make sure that the plotted densities correspond with your expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normal_pdf(x: np.array, m: float, s: float):\n",
        "    \"\"\"\n",
        "    Computes the Probability Density Function.\n",
        "\n",
        "    Input:\n",
        "    x - Numpy Array\n",
        "    m - Mean\n",
        "    s - Standard Deviation\n",
        "\n",
        "    Output:\n",
        "    - A Numpy array with the Gaussian PDF function applied to the content.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def x_axis():\n",
        "    \"\"\"\n",
        "    Creates the proper X-axis to return.\n",
        "\n",
        "    Output:\n",
        "    - A Numpy linspace array with the correct dimensions.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "x_grid = x_axis()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# plot density functions (Fill in the ???)\n",
        "#plt.plot(x_grid, ???, \"-\", lw=5, label=\"Class #1 (Setosa)\")\n",
        "#plt.plot(x_grid, ???, \"-\", lw=5, label=\"Class #2 (Versicolour)\")\n",
        "#plt.plot(x_grid, ???, \"-\", lw=5, label=\"Class #3 (Virginica)\")\n",
        "\n",
        "plt.xlabel(\"Petal Length\", fontsize=25)\n",
        "plt.ylabel(r\"$\\mathcal{N}(\\mu, \\sigma^2)$\", fontsize=25)\n",
        "plt.legend(loc=0, fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-15-markdown",
      "metadata": {},
      "source": [
        "## Posterior probabilities [2 pts]\n",
        "\n",
        "The plot above shows the probability densities for a value $x$, provided that you know the parameters for a specific class $C_i$, i.e. $p(x | \\mu_i, \\sigma_i)$. Because the mean and standard deviation are suffient statistics for normally distributed data, this is equivalent to the probability density given that specific class $p(x | C_i)$. However, what would be useful for classification, is the posterior probabilities of the classes given the data, i.e. $P(C_i | x)$. We can use Bayes' rule for this\n",
        "\n",
        "(3.5) $$P(C_i | x) =  \\frac{p(x | C_i) P(C_i)}{p(x)} = \\frac{p(x | C_i) P(C_i)}{\\sum_{k=1}^K p(x | C_k) P(C_k)}$$\n",
        "\n",
        "Because here we have no prior knowledge of the distribution of the different classes, we can just assume all prior class probabilities $P(C_i)$ to be equal. For our 3 class problem, that would mean a probability of $\\frac{1}{3}$ for each class, but we can also just factor the common prior out of the equation and simplify to\n",
        "\n",
        "$$P(C_i | x) = \\frac{p(x | C_i)}{\\sum_{k=1}^K p(x | C_k)}$$\n",
        "\n",
        "In order to make the code a little easier to write, make a vector $m$ with all values of $m_i$ and vector $s$ with all values of $s_i$, where $m_i$ and $s_i$ are the estimates for the class $C_i$. You can then just use those 2 vectors and a parameter $i$ to indicate which class you want to compute the posterior for. Write the function for the `posteriors`. It should take as input a data array and return the posterior matrix of dimensions $N \\times |C|$ where $N$ is the number of input points and $|C|$ is the total number of classes (3 in our case). Plot the posterior probabilities for all 3 classes. Does the plot of these 3 posteriors make sense based on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def posteriors(x, m, s):\n",
        "    \"\"\"\n",
        "    Computes the Posterior probabilities for each class.\n",
        "\n",
        "    Input:\n",
        "    x -  a Numpy array containing the data.\n",
        "    m -  a list containing the mean for each class.\n",
        "    s -  a list containing the std for each class.\n",
        "\n",
        "    Output:\n",
        "    -A Numpy array containing the posterior probs for each class, for each data point\n",
        "    NOTE: The rows of the output matrix should sum to 1!\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def vectorize_values(*args):\n",
        "    \"\"\"\n",
        "    Vectorizes the given input arguments into 1 vector.\n",
        "\n",
        "    Input:\n",
        "    args - A tuple of variable amount of input arguments.\n",
        "\n",
        "    Output:\n",
        "    - A vector containing these values.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# get posteriors (Fill in the ???)\n",
        "x_grid = np.linspace(0, 7, 1000)\n",
        "#posts = ???\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# plot posterior probability for each class\n",
        "#plt.plot(x_grid, posts[:,0], \"-\", lw=5, label=\"Class #1 (Setosa)\")\n",
        "#plt.plot(x_grid, posts[:,1], \"-\", lw=5, label=\"Class #2 (Versicolour)\")\n",
        "#plt.plot(x_grid, posts[:,2], \"-\", lw=5, label=\"Class #3 (Virginica)\")\n",
        "\n",
        "plt.xlabel(\"Petal Length\", fontsize=25)\n",
        "plt.ylabel(r\"$P(C_i | x)$\", fontsize=25)\n",
        "plt.legend(loc=2, fontsize=15)\n",
        "plt.ylim([0, 1.5])\n",
        "plt.xlim([0, 7])\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-17-markdown",
      "metadata": {},
      "source": [
        "## Bayes Classifier [2 pts]\n",
        "\n",
        "Now that we can compute the posteriors for every class, constructing a classifier is easy. The Bayes classifier is defined as\n",
        "\n",
        "(3.6) $$choose\\ C_i\\ if\\ P (C_i |x) = max_k\\ P(C_k |x)$$\n",
        "\n",
        "Write the code for the `classify` function. It should classify a single data point $x$ as one of the 3 classes, returning $0$, $1$ or $2$ based on the which class the flower is most likely to belong to. The other arguments of the function should therefore be the vector of mean estimates $m$ and the vector of standard deviation estimates $s$.\n",
        "\n",
        "Next complete the `validate` function below. It should take a validation set, the expected class for all data points in that set and the vectors $m$ and $s$ with which to make the classifications. Based on this, it should return the percentage of elements in the validation set that were classified correctly.\n",
        "\n",
        "Remember that the mean and standand deviation estimates we have computed are only based on the *petal length* variable, while a data point in the validation set consists of a complete row, i.e. all 4 variables describing the dimensions of a flower instance. So, for now, you will only need to use the *petal length* variable from each data point to attempt to classify it.\n",
        "\n",
        "Apply the `validate` function to all 3 validation sets and report the percentage correct for each class. Also include the average correct percentage for all 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify(x, m, s):\n",
        "    \"\"\"\n",
        "    Classify the input data using your previously defined posteriors() function!\n",
        "\n",
        "    Input:\n",
        "    x - The input data.\n",
        "    m - The vectorized means.\n",
        "    s - The vectorized STDs.\n",
        "\n",
        "    Output:\n",
        "    A Numpy array containing the predicted targets from your posteriors.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def validate(val, expected, m, s):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of classification against the validation set.\n",
        "\n",
        "    Input:\n",
        "    val - validation input data.\n",
        "    expected - the correct targets for each position.\n",
        "    m - The vectorized means.\n",
        "    s - The vectorized STDs.\n",
        "\n",
        "    Output:\n",
        "    A float representing the accuracy against the validation set.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# YOUR SOLUTION HERE (fill in the ???)\n",
        "\n",
        "#print(\"C0: Accuracy = %.3f\" %(???))\n",
        "#print(\"C1: Accuracy = %.3f\" %(???))\n",
        "#print(\"C2: Accuracy = %.3f\" %(???))\n",
        "#print(\"Avg. Accuracy = %.3f\" %(???))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-19-markdown",
      "metadata": {},
      "source": [
        "# Multivariate distributions\n",
        "\n",
        "You should already get pretty good results using just the *petal length* variable (depending on the splits that were made for the validation sets, the exact percentage might vary a little), but maybe we can do better still. Looking at the original plots of the data, you might conclude that there could be more information for the classification in the other variables, so a multi-variate approach would make sense. This means we will need a multi-variate version of the probability density function for normal distributions, which is defined as\n",
        "\n",
        "(5.9) $$p(\\boldsymbol{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} e^{-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu})}$$\n",
        "\n",
        "where $\\boldsymbol{x}$ is a $d$-dimensional vector drawn from the normal distribution $\\mathcal{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$.\n",
        "\n",
        "If we used all 4 variables in the dataset to compute the vector of means and the covariance matrix for each class, then we could use the above equation to compute the probability densities for each class, with those the posteriors for each class and finally use the same Bayes classifier to select the most probable class for a data point. However if the number of variables is large, then the number parameters for the covariance matrix (sized $d \\times d$) can become too large to properly fit to the amount of data we have. It is therefore common to make some simplifying assumptions, to decrease the complexity of the model. One well know variant is *Naive Bayes*, where the assumption is that all variables are independent. This means the model only uses the variance for each variable and no covariances are included, which corresponds to a covariance matrix with only entries on the diagonal.\n",
        "\n",
        "## Naive bivariate distributions [4 pts]\n",
        "\n",
        "We will start with a bi-variate (i.e. 2 variables) distribution, as this will be easier to plot, visualize, and thus reason about. Looking at the original 2 plots of the data, the second plot using the *petal length* and *petal width* seems to result in nice clusters, so will use those 2 variables.  We will use the *Naive Bayes* model for our bivariate data. Even if computing the covariance might not be problematic in this case, it will still be useful to see the effect of such a simplifying assumption.\n",
        "\n",
        "This assumption of independence might be *naive* using real world data, you might even expect *length* and *width* of flower petals to be strongly correlated, but it turns out *Naive Bayes* can still make an effective classifier. The assumption of independence also simplifies the equation a lot: Instead of writing out the full version using the covariance matrices, we can write a multi-variate distribution as a product of one-dimensional distributions for each variable. So for our 2 variable case:\n",
        "\n",
        "$$p(\\boldsymbol{x}| C_i) = p(x_1 | C_i) p(x_2 | C_i)$$\n",
        "\n",
        "Where $p(x_1 | C_i)$ is the old univariate distribution using the $\\mu_1$ and $\\sigma_1$ parameters of the first variable for class $C_i$ and $p(x_2 | C_i)$ is the same equation using the $\\mu_2$ and $\\sigma_2$ parameters of the second variable for class $C_i$.\n",
        "\n",
        "Start by writing this *naive* version of the probability density function for the bivariate case. You will also have to compute the 3 means (for each of the 3 classes) and 3 standard deviations for the additional *petal width* variable. You should already have the code to compute these for the *petal length* variable from the univariate distributions.\n",
        "\n",
        "Now we will plot the density for each class to see what they look like. These plots will be a 3-dimensional; 2 dimensions for the values of *petal length* and *petal width*, and 1 dimension for the probability density belonging to that combination of variables. There are several ways to make 3d plots in *matplotlib*; we will use contour plots, as that is also what is used most in the book (see figure 5.3, 5.4, 5.5 and 5.6). Start with a simple plot of the point values of the 2 variables using a different color for each class (like at the start of the assignment) and overlay the 3 contours on top of them.\n",
        "\n",
        "Construct a mesh of X and Y values using [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) for each dimension and combining them with [meshgrid](https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html). Then compute the Z values for a probability density function of 1 class and use the [contour](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html) function to plot those Z values. Repeat this for the other 2 classes and show the final plot. You can also look at some demo uses of contour [here](https://matplotlib.org/stable/gallery/images_contours_and_fields/contour_demo.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def naive_bivar_normal_pdf(x1, x2, m1, m2, s1, s2):\n",
        "    \"\"\"\n",
        "    Computes the Naive bivariate normal probability density function.\n",
        "\n",
        "    Input:\n",
        "    x1 - data input for the first class\n",
        "    x2 - data input for the second class\n",
        "    m1 - mean input for the first class\n",
        "    m2 - mean input for the second class\n",
        "    s1 - standard deviation input for the first class\n",
        "    s2 - standard deviation input for the second class\n",
        "\n",
        "    Output:\n",
        "    - A Numpy Array containing the PDF's for the bivariate case.\n",
        "    *Hint: Re-use/Repurpose the functions you defined earlier.*\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def compute_mean_multiclass(C: np.array, variable: int):\n",
        "    \"\"\"\n",
        "    Computes the means of the input array.\n",
        "\n",
        "    Input:\n",
        "    C - Numpy array\n",
        "    variable - Int\n",
        "\n",
        "    Output:\n",
        "    Numpy array of 2-class means with shape [C x 2]\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def compute_std_multiclass(C: np.array, variable: int):\n",
        "    \"\"\"\n",
        "    Computes the standard deviations of the input array.\n",
        "\n",
        "    Input:\n",
        "    C - Numpy array\n",
        "    variable - Int\n",
        "\n",
        "    Output:\n",
        "    Numpy array of 2-class STDs with shape [C x 2]\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# make mesh grid\n",
        "grid_size = 1000\n",
        "x = np.linspace(1, 6.5, grid_size)\n",
        "y = np.linspace(-.5, 2.5, grid_size)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "# NOTE: X and Y will be of size (grid_size X grid_size).\n",
        "# Use ravel() (or a similar function) to make them 1-dim\n",
        "\n",
        "# compute densities (Fill in the ???)\n",
        "\n",
        "#pdf_C0 = ???\n",
        "#pdf_C1 = ???\n",
        "#pdf_C2 = ???\n",
        "\n",
        "# make contour plot\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# plot data (Fill in the ???)\n",
        "#???\n",
        "#???\n",
        "#???\n",
        "\n",
        "# plot density contours (Fill in the ???)\n",
        "#cp0 = plt.contour(X, Y, ???, levels=10, cmap=\"Reds\")\n",
        "#cp1 = plt.contour(X, Y, ???, levels=10, cmap=\"Blues\")\n",
        "#cp2 = plt.contour(X, Y, ???, levels=10, cmap=\"Greens\")\n",
        "\n",
        "plt.legend(loc=4, fontsize=18)\n",
        "plt.xlabel(\"Petal Length\", fontsize=25)\n",
        "plt.ylabel(\"Petal Width\", fontsize=25)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-21-markdown",
      "metadata": {},
      "source": [
        "## Naive classifier [4 pts]\n",
        "\n",
        "Now that you have the naive bivariate version of the pdf fuctions, constructing the *Naive Bayes* classifier will be straightforward. Simply adapt your previous `posteriors`, `classify` and `validate` functions to work with your `naive_bivar_normal_pdf` function.\n",
        "\n",
        "Show the validation results for each of the 3 classes and the average over all 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def posteriors_naive(x1, x2, m1, m2, s1, s2):\n",
        "    \"\"\"\n",
        "    Computes the Naive Posterior probabilities for each class.\n",
        "\n",
        "    Input:\n",
        "    x1 - data input for the first class\n",
        "    x2 - data input for the second class\n",
        "    m1 - mean input for the first class\n",
        "    m2 - mean input for the second class\n",
        "    s1 - standard deviation input for the first class\n",
        "    s2 - standard deviation input for the second class\n",
        "\n",
        "    Output:\n",
        "    -A Numpy array containing the Naive posterior probs for each class, for each data point\n",
        "    NOTE: The rows of the output matrix should sum to 1!\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def classify_naive(x1, x2, m1, m2, s1, s2):\n",
        "    \"\"\"\n",
        "    Computes the Naive Posterior probabilities for each class.\n",
        "\n",
        "    Input:\n",
        "    x1 - data input for the first class\n",
        "    x2 - data input for the second class\n",
        "    m1 - mean input for the first class\n",
        "    m2 - mean input for the second class\n",
        "    s1 - standard deviation input for the first class\n",
        "    s2 - standard deviation input for the second class\n",
        "\n",
        "    Output:\n",
        "    - A Numpy array containing the predicted classes for each data point.\n",
        "    NOTE: Make sure to re-use your posteriors_naive() function!\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def validate_naive(val, expected, m1, m2, s1, s2):\n",
        "    \"\"\"\n",
        "    Computes the accuracy of classification against the validation set.\n",
        "\n",
        "    Input:\n",
        "    val - validation input data.\n",
        "    expected - the correct targets for each position.\n",
        "    m1 - The vectorized means of the first class.\n",
        "    s1 - The vectorized STDs of the first class.\n",
        "    m2 - The vectorized means of the second class.\n",
        "    s2 - The vectorized STDs of the second class.\n",
        "\n",
        "    Output:\n",
        "    A float representing the accuracy against the validation set.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# YOUR SOLUTION HERE (Fill in the ???)\n",
        "#print(\"C0: Accuracy = %.3f\" %(???))\n",
        "#print(\"C1: Accuracy = %.3f\" %(???))\n",
        "#print(\"C2: Accuracy = %.3f\" %(???))\n",
        "#print(\"Avg. Accuracy = %.3f\" %(???))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-23-markdown",
      "metadata": {},
      "source": [
        "## Decision boundaries [1 pt]\n",
        "\n",
        "In *Alpaydin* the posterior equations are simplified as much as possible into a discriminant function, while still retaining the property\n",
        "\n",
        "$$g_i(x) > g_j(x) \\iff P(x|C_i) > P(x|C_j)$$\n",
        "\n",
        "This can simplify the computation, or help gain insight in the actual distinguishing elements of the particular model. With some algebra, they can also be written into equations for the decision boundaries (the line where $g_i(x) = g_j(x)$). As we have a computer at our disposal, we won't worry too much about how many computations are needed and just let the machine do the work. If you make a contour plot of the complete posterior, these contours should also meet at the decision boundary, while being a lot less work to program.\n",
        "\n",
        "For the previous cell you already wrote the `posteriors_naive` function, so all that is left is to make contour plots just like for the `naive_bivar_normal_pdf` function. Show the points for the 3 classes again and overlay the 3 posterior contours on top. Do the decision regions look like you would expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR SOLUTION HERE\n",
        "\n",
        "# compute densities\n",
        "#???\n",
        "\n",
        "# make contour plot\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# plot data\n",
        "#???\n",
        "#???\n",
        "#???\n",
        "\n",
        "# plot posterior contours\n",
        "#cp0 = plt.contour(X, Y, ???, levels=2, cmap=\"Reds\")\n",
        "#cp1 = plt.contour(X, Y, ???, levels=2, cmap=\"Blues\")\n",
        "#cp2 = plt.contour(X, Y, ???, levels=2, cmap=\"Greens\")\n",
        "\n",
        "plt.legend(loc=4, fontsize=18)\n",
        "plt.xlabel(\"Petal Length\", fontsize=25)\n",
        "plt.ylabel(\"Petal Width\", fontsize=25)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-25-markdown",
      "metadata": {},
      "source": [
        "## Full bivariate distribution [3 pts]\n",
        "\n",
        "Finally, we will compare this naive bivariate distribution with a full bivariate distribution, using the complete covariance matrix. In order to skip programming out the complete equations for this, you may use a built-in function, namely [multivariate normal distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) from the *SciPy* stats module. With this we can create a distribution based on a mean vector and a covariance matrix. This distribution can then be used to compute the probability density for a vector of x values, like so:\n",
        "\n",
        "    distribution = multivariate_normal(mean=mean_vector, cov=covariance_matrix)\n",
        "    prob_density = distribution.pdf(x_vector)\n",
        "\n",
        "The mean vector and covariance matrix for a class can be computed using the *Numpy* functions [mean](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html) and [covariance](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html).\n",
        "\n",
        "Create 3 distributions, one for each class, using the functions linked above. Write the `full_bivar_normal_pdf` function, which takes a `distr` argument that should correspond to one distribution for a class. Use that distribution to compute the probability density of the pair `x1`, `x2`.\n",
        "\n",
        "Again plot the points for the 3 classes, using different colors for each class and overlay the contours of the probability densities for each of the 3 classes using a meshgrid. This should be the same as your contour plot for the naive bivariate distribution, only now using the full bivariate distribution to compute the densities.\n",
        "\n",
        "*What is the difference between these 2 plots? Why?*\n",
        "\n",
        "__YOUR ANSWER HERE__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def full_bivar_normal_pdf(x1, x2, distr):\n",
        "    \"\"\"\n",
        "    Computes the probability density of X1 & X2 using the given distribution's PDF function.\n",
        "\n",
        "    Input:\n",
        "    x1 - X-axis datapoints.\n",
        "    x2 - X-axis datapoints.\n",
        "    distr - The distribution to use its PDF from.\n",
        "\n",
        "    Output:\n",
        "    - The PDF applied output of the stacked input data.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def create_distribution(mean: np.array, data: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    Creates the multivariate normal distribution.\n",
        "\n",
        "    Input:\n",
        "    mean - Numpy array featuring means for the class.\n",
        "    data - Numpy array featuring the input data for the covariance matrix.\n",
        "\n",
        "    Output:\n",
        "    - The multivariate distribution for the given class.\n",
        "    \"\"\"\n",
        "    from scipy.stats import multivariate_normal\n",
        "    raise NotImplementedError\n",
        "\n",
        "# define distributions (Fill in the ???)\n",
        "#distribution_C0 = ???\n",
        "#distribution_C1 = ???\n",
        "#distribution_C2 = ???\n",
        "\n",
        "# compute density functions (Fill in the ???)\n",
        "#pdf_C0 = ???\n",
        "#pdf_C1 = ???\n",
        "#pdf_C2 = ???\n",
        "\n",
        "# make contour plot\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# plot data (Fill in the ???)\n",
        "#???\n",
        "#???\n",
        "#???\n",
        "\n",
        "# plot density contours (Fill in the ???)\n",
        "#cp0 = plt.contour(X, Y, ???, levels=10, cmap=\"Reds\")\n",
        "#cp1 = plt.contour(X, Y, ???, levels=10, cmap=\"Blues\")\n",
        "#cp2 = plt.contour(X, Y, ???, levels=10, cmap=\"Greens\")\n",
        "\n",
        "plt.legend(loc=4, fontsize=18)\n",
        "plt.xlabel(\"Petal Length\", fontsize=25)\n",
        "plt.ylabel(\"Petal Width\", fontsize=25)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cell-27-markdown",
      "metadata": {},
      "source": [
        "## Full bivariate classifier [3 pts]\n",
        "\n",
        "Adapt your `posteriors`, `classify` and `validate` functions to work your `full_bivar_normal_pdf` function. Each function should take a `distrs` argument, which should be a list containing the 3 `multivariate_normal` distributions for the 3 classes.\n",
        "\n",
        "Show the validation results for each of the 3 classes and the average over all 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-28-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def posteriors_full(x1, x2, distrs):\n",
        "    \"\"\"\n",
        "    Computes the posteriors of the full bivariate classifier.\n",
        "\n",
        "    Input:\n",
        "    x1 - The input data for the first class.\n",
        "    x2 - The input data for the second class.\n",
        "    distrs - The multivariate normal distribution.\n",
        "\n",
        "    Output:\n",
        "    - A Numpy array containing the posteriors applied to the input data.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def classify_full(x1, x2, distrs):\n",
        "    \"\"\"\n",
        "    Clasify the input data for both classes based on the given distribution.\n",
        "\n",
        "    Input:\n",
        "    x1 - Input data for the first class.\n",
        "    x2 - Input data for the second class.\n",
        "    distrs - Distribution used to classify the data with.\n",
        "\n",
        "    Output:\n",
        "    - A Numpy array containing the estimates for the classes.\n",
        "    NOTE: Use the posteriors_full() function!\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def validate_full(val, expected, distrs):\n",
        "    \"\"\"\n",
        "    Validates the predicted targets, using the distribution, \n",
        "    of the input against the validation set to compute the accuracy.\n",
        "\n",
        "    Input:\n",
        "    val - Validation input data.\n",
        "    expected - Predicted targets of the classification.\n",
        "    distrs - Given distribution used in classification.\n",
        "\n",
        "    Output:\n",
        "    The accuracy of the output measured against the validation set.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# YOUR SOLUTION HERE\n",
        "#acc_C0 = ???\n",
        "#acc_C1 = ???\n",
        "#acc_C2 = ???\n",
        "\n",
        "#print(\"C0: Accuracy = %.3f\" %(???))\n",
        "#print(\"C1: Accuracy = %.3f\" %(???))\n",
        "#print(\"C2: Accuracy = %.3f\" %(???))\n",
        "#print(\"Avg. Accuracy = %.3f\" %(???))"
      ]
    }
  ],
  "metadata": {
    "cgVersion": 1,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
